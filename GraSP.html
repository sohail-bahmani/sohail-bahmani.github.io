<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<!-- #BeginTemplate "template.dwt" -->

<head>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
<meta name="viewport" content="width=device-width, initial-scale=1.0"/>
<link href='https://fonts.googleapis.com/css?family=Catamaran:300' rel='stylesheet' type='text/css'/>
<link href="default.css" rel="stylesheet" type="text/css" />
<title>Sohail Bahmani</title>
<script src="ga-script.js"></script>
<!-- #BeginEditable "doctitle" -->
<script type="text/x-mathjax-config">
        MathJax.Hub.Config({ 
        TeX: { equationNumbers: {autoNumber: "all"} }        
        });
      </script>
      <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- #EndEditable -->
</head>

<body>


<div id="background" class="mainFrame">
	<span class="navwrap">
            <div id="menubar" class="row">
                <a class="menu" id="homemenu" href="index.html#home"></a>
                <div style="background-color: rgb(2, 209, 119);" class="vsep"></div>
                <a class="menu" href="index.html#research">Research</a>                
                <div style="background-color: rgb(64, 176, 225);" class="vsep"></div>
                <a class="menu" href="index.html#talks">Talks</a>
                <div style="background-color: rgb(2, 209, 119);" class="vsep"></div>
                <a class="menu" href="index.html#publications">Publications</a>
                <div style="background-color: rgb(64, 176, 225);" class="vsep"></div>
                <a class="menu marked" href="./CV.pdf">CV</a>
            </div>
    </span>
	<div id="maincolumn" class="column">
	<!-- #BeginEditable "MainText" -->
		<div class="item">
		<h4>Gradient Support Pursuit (GraSP)</h4>
		<p>This algorithm can be used as an approximate solver for sparsity 
			constrained optimization problems. The algorithm generalizes the CoSaMP 
			algorithm used in Compressed Sensing, and can handle cost functions that 
			are non-quadratic. <span class="warn">Note that our implementation of GraSP is 
			NOT optimized for speed or memory efficieny</span>. Please contact me 
			via email in case you encounter any bugs in the code. <a class="hlink marked" href="Codes/GraSP.zip">Download</a></p> 
		<!--</div>
		<div class="item">-->
		<h5 class="highlight">Pseudo-Code of the Algorithm</h5>
		<table>
			<tr>
				<td colspan="2" style="height: 25px">\(k=0\)</td>
			</tr>
			<tr>
				<td>\(\mathbf{x}^{\left(k\right)}=\mathbf{0}\)</td>
			</tr>
			<tr>
				<td colspan="2">Repeat</td>
			</tr>
			<tr>                
				<td style="float: left; width: 60%;">
                    \begin{align}
                        \mathbf{z}^{\left(k\right)} & =\nabla f\left(\mathbf{x}^{\left(k\right)}\right)  \nonumber \\                        \mathcal{T}_k & =\text{supp}\left(\mathbf{x}^{\left(k\right)}\right)\cup\text{supp}\left(\mathbf{z}_{2s}^{\left(k\right)}\right)  \nonumber \\
                         \mathbf{b}^{\left(k\right)} & =\operatorname*{argmin}_{\mathbf{x}}\ f\left(\mathbf{x}\right)\,,\hspace{2em}\text{subject to}\ \left.\mathbf{x}\right\vert_{\mathcal{T}_k^\mathrm{c}}=\mathbf{0} \hspace{2em}\label{iopt} \tag{*}\\
                         \mathbf{x}^{\left(k+1\right)} & =\mathbf{b}^{\left(k\right)}_s  \nonumber \\
                        k & \leftarrow k+1  \nonumber
                    \end{align}
                </td>
			</tr>					
			<tr>
			<td colspan="2">Until <em>halting condition</em> holds</td>
			</tr>
			<tr>
			<td colspan="2">Return \(\mathbf{x}^\left(k\right)\)</td>
			</tr>
		</table>
		<!--</div>
		<div class="item">-->
		<h5 class="highlight" id="modes">Algorithm Modes</h5>
		<p> In addition to the original form of the algorithm 
		described by the pseudo-code, we also implemented two other variants of the algorithm that substitute the inner optimization 
		step by computationally simpler steps. The 
		algorithm is implemented with the following modes.</p>
		<ol>
			<li><strong>Restricted Minimization:</strong> In this mode the inner optimization is performed 
			as described by the pseudo-code. This mode has the highest 
			computational cost per iteration, though it is more stable than the 
			other two methods. For the inner optimization step performed 
			in this mode (i.e., \eqref{iopt} in the pseudo-code) 
		we rely on <a href="https://www.cs.ubc.ca/~schmidtm/">Mark Schmidt</a>'s convex 
		optimization packages, <a href="https://www.cs.ubc.ca/~schmidtm/Software/minFunc.html">minFunc</a>, and <a href="https://www.cs.ubc.ca/~schmidtm/Software/minConf.html">minConf</a>. <span class="warn">Make sure that these packages are 
		accessible to GraSP by including their directories in the MATLAB path</span>. 
			Supplying a separate Hessian-vector multiplication function, or using a completely Hessian-free optimization technique to solve \eqref{iopt} can improve the speed in this mode.</li>			
			<li><strong>Restricted Gradient Descent:</strong> In this mode, the inner optimization is 
			simply replaced by a gradient descent step for the restriction of 
			the cost function to the coordinates in <code><i>T</i></code>. 
			It is easy to see that in this mode the algorithm would be equivalent 
			to a gradient descent with hard thresholding. This mode has the 
			lowest computational complexity per iteration.</li>
			<li><strong>Restricted Newton's Method:</strong> In this mode in each 
			iteration of the 
			algorithm the crude estimate <code><b>b</b></code> is obtained by 
			taking a newton step for the restriction of the cost function to the 
			coordinates in <code><i>T</i></code>, rather than minimizing the cost function 
			over the vectors whose support is a subset of <code><i>T</i></code>. Computational cost per iteration in this mode
			is less than that in the full mode.</li>
		</ol>
		<!--</div>
		<div class="item">-->
		<h5 class="highlight">Bounded Iterates / \(\ell_2\)-regularized Iterate</h5>
			<p>Generally for non-quadratic cost functions one needs to explicitly bound the iterates or augment the cost function by an \(\ell_2\)-regularization term. 
			The implementation of GraSP incorporates these two formulations as 
			well.</p>
		<!--</div>
		<div class="item">-->
			<h5 class="highlight">How to Use GraSP in MATLAB?</h5>
			<p>To run the algorithm you can call
			<table align="center">
			<tr>			
			<td><code>xhat = GraSP(F,s,n,options);</code></td>
			</tr></table> in MATLAB. The input arguments are described below:</p>
			<p><code>F :</code> This is a function handle that allows GraSP to access the cost function and its derivatives. <span class="warn">Note that in general the function <code>F</code> should take two inputs and return three 
			outputs as in <code>[f, <strong>g</strong>, <strong>H</strong>] = F(<strong>x</strong>,<i>T</i>)</code></span>.
			In a nutshell, it will return the value (<code>f</code>), the gradient (<code><strong>g</strong></code>), and the Hessian (<code><strong>H</strong></code>) of the cost function at
			<code><strong>x</strong></code> when the cost function is restricted 
			to the set of coordinates enumerated by <code><i>T</i></code>.&nbsp; 
			In the &ldquo;full&rdquo; and &ldquo;gradient&rdquo; modes of the GraSP algorithm, supplying the Hessian is optional as it is not required by the algorithm. However, in 
			most cases having access to the Hessian improves the convergence 
			speed of the inner (convex) optiomization step in the &ldquo;full&rdquo; mode.</p>
			<p><code>s</code> : A positive integer that determines the desired sparsity level.</p>
			<p><code>n</code> : The dimensionalty of the solution.</p>
			<p><code>options</code> : This is an optional structure with the 
			following fields that can be used to configure GraSP.</p>
			<p class="auto-style1"><code>.HvFunc</code> :&nbsp; This is a 
			function handle for a user-supplied Hessian-vector multiplication 
			that can be used by solver of \eqref{iopt}. In our 
			implementation, for a cost function \(g\left(\cdot\right)\), <code>HvFunc(<strong>v</strong>,<strong>x</strong>)</code> 
			is supposed to evaluate the Hessian-vector poduct \(\nabla^2 g\left(\mathbf{x}\right)\mathbf{v}\). 
			This field is optional.</p>
			<p class="auto-style1"><code>.maxIter</code> : The maximum number of 
			iterations for GraSP. The default value is 100.</p>
			<p class="auto-style1"><code>.tolF :</code> GraSP will halt if the value of the cost function at the current iterate is less than
			<code>tolF</code>. The default value is 1e-3.</p>

			<p class="auto-style1"><code>.tolG</code> : GraSP will halt if restriction 
			of the gradient to its \(3s\)-largest entries has an \(\ell_2\)-norm less than <code>tolG</code>. The 
			default value is 1e-3.</p>
			<p class="auto-style1"><code>.mu</code> : This parameter allows \(\ell_2\)-regularization of the cost function.
			 If <code>mu</code> is positive it determines the 
			radius of the sphere of the feasible points in the constrained form. 
			If <code>mu</code> is less than or equal to zero, it determines the penalty 
			coefficient in the penalized form. The default value for this 
			argument is zero.</p>
			<p class="auto-style1"><code>.Method</code> : This agument takes values 'F', 'G', 
			or 'H' to select the <a href="#modes">modes</a> 1,2, or 3 of the algorithm described 
			above. The default value is 'F'.</p>
			<p class="auto-style1"><code>.eta</code> : If GraSP runs in the Restricted Gradient or Hessian modes (i.e., <code>.Mode</code> is set to 'G' or 'H'),
			 then the associated step-size is determined by the positive number <code>eta</code>. The default value is one.</p>
			<p class="auto-style1"><code>.debias</code> : A logical variable that indicates whether or not the debiasing is performed over the estimated support set at the end of each iteration. The default value is false.</p>
			
			</div>	
			
	<!-- #EndEditable -->	
	</div>
</div>
</body>
<!-- #EndTemplate -->
</html>
