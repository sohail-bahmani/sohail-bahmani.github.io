<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<!-- #BeginTemplate "template.dwt" -->

<head>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
<link href="default.css" rel="stylesheet" type="text/css" />
<title>Sohail Bahmani</title>
<script>
		  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
		  ga('create', 'UA-43731016-1', 'gatech.edu');
		  ga('send', 'pageview');
</script>
<!-- #BeginEditable "doctitle" -->
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<!-- #EndEditable -->
</head>

<body>





<div id="background" class="mainFrame">
	<div id="menubar" class="row">
		<div class="vsep" style="background-color: #40B0E1"></div>
		<div class="menu" onclick="window.location.href='./index.html'">About</div>
		<!--<div class="vsep" style="background-color: #999933"></div>
		<div class="menu" onclick="window.location.href='./research.html'">Research</div>-->
		<div class="vsep" style="background-color: #990000"></div>
		<div class="menu" onclick="window.location.href='./publications.html'">Publications</div>		
		<div class="vsep" style="background-color: #D17702"></div>
		<div class="menu" onclick="window.location.href='./CV.pdf'">CV</div>
	</div>
	<div id="maincolumn" class="column">	
	<!-- #BeginEditable "MainText" -->
	<div class="item">
	<h4>Sparsity-Constrained Optimization</h4>
		<p>The goal of this project is to develop iterative algorithms for 
		sparsity-constrained optimization. The generic formulation of these 
		problems is expressed by <a class="equation" name="P0">\[\arg\min_\mathbf{x}\ 
		f\left(\mathbf{x}\right)\quad \text{subject to } 
		\left\Vert\mathbf{x}\right\Vert_{0}\leq s,\tag{P0}\]</a> where 
		\(f:\mathbb{R}^p\longrightarrow\mathbb{R}\) is a given objective 
		function and \(s\) is the presumed sparsity level.</p>
		<p>Optimization problems of form <a href="#P0">(P0)</a> 
		appear in variety of application. The most natural example of these 
		applications is the <em>feature selection</em> problems in Machine 
		Learning and Statistics. Given a relatively small number of samples from 
		a high-dimensional data and their associated labels, in these problems 
		one seeks a small number of features using which the data can be 
		categorized most accurately. This framework applies to variety of 
		problems in areas ranging from genomics to economics. Depending on the 
		application different forms of loss functions (i.e. objective functions) 
		are employed in these areas to measure the discrepancy of the estimate 
		against some given observations.</p>
		<p>Note that <a href="#P0">(P0)</a> in general poses an NP-hard problem 
		even in cases that \(f\left(\cdot\right)\) has a very simple form. For 
		example, for quadratic objective functions of form 
		\(f\left(\mathbf{x}\right)=\left\Vert\mathbf{Ax}-\mathbf{y}\right\Vert^2_2\) 
		the minimization in <a href="#P0">(P0)</a> that arise in <em>
		<a href="http://en.wikipedia.org/wiki/Compressed_sensing">Compressed Sensing</a></em> 
		(CS) as an ideal estimation mechanism known to be NP-hard. Therefore, it 
		is&nbsp; reasonable to focus on approximate solvers of <a href="#P0">(P0)</a>.</p>
		<p>Approximate solvers of <a href="#P0">(P0)</a>&nbsp; are mostly 
		developed and studied in the field of CS where quadratic objective functions 
		are of interest. There are also a few studies about the approximate solvers of <a href="#P0">(P0)</a> 
		in the framework of statistical estimation where the objective function 
		can have certain non-quadratic form (e.g., logistic loss, Poisson loss, 
		etc). However, these algorithms mostly rely on convex relaxation with 
		\(\ell_1\)-norm which is not guaranteed to yield exactly sparse 
		solutions which is critical in applications such as feature selection. A 
		few greedy algorithms has also been proposed, but they either do not 
		provide stand-alone accuracy guarantees or have stringent criteria for 
		the objective function.</p>
		<p>The main goal is to provide approximate solvers for <a href="#P0">(P0)</a> 
		that:</p>
		<ol>
			<li>have provable accuracy guarantees,</li>
			<li>rely on simple and not very stringent assumptions about the 
			objective function, and</li>
			<li>scale well with dimensionality of the problem so that they 
			remain tractable.</li>
		</ol>
	<!--	</div>
	<div class="item">-->
	<h4>Non-Convex Optimization for Compressed Sensing</h4>
		<p>The state-of-the-art algorithms in CS use the \(\ell_1\)-norm as a 
		convex surrogate for the \(\ell_0\)-norm to obtain a convex optimization 
		problem that is tractable to solve. However, this relaxation result in a 
		suboptimal algorithm in terms of the undersampling factor. It is known 
		that non-convex approximations of \(\ell_0\)-norm such as 
		\(\ell_p\)-quasinorms (with \(0&lt;p&lt;1\)) are superior to \(\ell_1\)-norm 
		approximations. However, since they are non-convex, convergence of the 
		algorithm to the global optimum is a challenge to establish. In this 
		project we investigate the non-convex formulations of the CS problems 
		for which we can provide global convergence guarantees.</p>
	<!--</div>
	<div class="item">-->
	<h4>Non-Linear Sparsity Models in Compressed Sensing</h4>
		<p>In standard CS problems the signal of interest that must estimated 
		from some linear measurements is assumed to be sparse with respect to a 
		basis or frame. The goal of this project is to generalize this framework 
		to allow sparsity with respect to non-linear maps. Many of the current 
		CS extensions such as model-based CS&nbsp; as well as new formats of CS 
		can be described in this generalized framework. In particular, the 
		generalized framework can express a broader range of structured-sparsity 
		models that occur in different applications. In this project we develop 
		the theory for this generalized CS framework that incorporates 
		non-linear sparsity models.</p>
	</div>
	<!-- #EndEditable -->	
	</div>
</div>
<!-- <span style="bottom: 0px; right: 0px; top: auto; left: auto; position: fixed">		
		<a title="Google Analytics Alternative" href="http://clicky.com/66622339"><img alt="Google Analytics Alternative" src="//static.getclicky.com/media/links/badge.gif" border="0" /></a>
		<!-- <script type="text/javascript" src="clicky.js"></script>-->			
		<!-- <script src="https://static.getclicky.com/js" type="text/javascript"></script>
		<script type="text/javascript">try{ clicky.init(66622339); }catch(e){}</script>
		<noscript><p><img alt="Clicky" width="1" height="1" src="//in.getclicky.com/66622339ns.gif" /></p></noscript>
</span>-->
</body>
<!-- #EndTemplate -->
</html>
