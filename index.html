<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<!-- #BeginTemplate "template.dwt" -->

<head> 
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-NBJNM3C');</script>
  <meta http-equiv="content-type" content="application/xhtml+xml; charset=UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link href="https://fonts.googleapis.com/css?family=Catamaran:400&display=swap" rel="stylesheet"> 
  <link href="default.css" rel="stylesheet" type="text/css" />
  <script type="text/javascript" src="accordion-pre.js"></script>
  <link type="text/css" rel="stylesheet" href="accordion.css" />
  <title>Sohail Bahmani</title>
  <!-- <script src="ga-script.js"></script>-->
  <script async src="https://ipmeta.io/plugin.js"></script>
  <!-- #BeginEditable "doctitle" -->
  <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "all"} } });
      </script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>

<body>
	<!-- Google Tag Manager (noscript) -->
	<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NBJNM3C"
	height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
	<!-- End Google Tag Manager (noscript) -->
  <div id="background" class="mainFrame">
    <span class="navwrap">
      <div id="menubar" class="row">
        <a class="menu" id="homemenu" href="#home"></a>
        <div style="background-color: rgb(2, 209, 119);" class="vsep"></div>
        <a class="menu" href="#research">Research</a>
        <div style="background-color: rgb(64, 176, 225);" class="vsep"></div>
        <!-- <a class="menu" href="#talks">Talks</a>
        <div style="background-color: rgb(2, 209, 119);" class="vsep"></div> -->
        <a class="menu" href="#publications">Publications</a>
        <div style="background-color: rgb(2, 209, 119);" class="vsep"></div> 
        <!-- <div style="background-color: rgb(64, 176, 225);" class="vsep"></div> -->
        <a class="menu marked" href="./CV.pdf">CV</a>
      </div>
    </span>
    <div id="maincolumn" class="column">
      <!-- #BeginEditable "MainText" -->
      <span id="home" class="anchor"></span>
      <div> <!--class="item">-->
        <table style="width: 100%;">
          <tbody>
            <tr>
              <td
                style="text-align: right; vertical-align: top; margin: 0px; padding: 0px; min-width: 2in; max-height: 40%;">
                <img border="0" align="left" style="display: block; width: 100%; height: auto; max-width: 2in; border-radius: 50%"
                  src="profilecsq.jpg" />
			  </td>
			  <td style="text-indent: 0px; min-width: 67%; text-align: left; vertical-align: top;">
                <h1 style="font-size:36pt;">Sohail Bahmani</h1>
              </td>
              
            </tr>
          </tbody>
        </table>
      </div>
	  <br/>
      <span id="research" class="anchor"></span>
      <div class="item">
        <h2>Research Vignette</h2>
        <!--- Robust Mean Estimation --->  
        <button class="accordion active">
            <div class="accordion-title marked">Robust Mean Estimation via Empirical Characteristic Function</div>
        </button>
          <div class="panel no-js">
              <p>Estimation of mean of a random variable is a fundamental problem of statistics since many estimation problems such as density estimation, regression, \(\ldots\), can be posed as mean estimation. The most benign and pedagogical example one can imagine is perhaps estimation of the mean of a random Gaussian vector in \(\mathbb{R}^d\), from a finite number of i.i.d. samples. The <i>sample mean</i>, the average of the observed samples, turns out to achieve the best possible <i>confidence interval</i> in the Gaussian setting. Unfortunately, the sample mean loses its glory as soon as we start dealing with heavy-tailed distributions or adversarially manipulated samples. In such unfavorable scenarios, the fundamental questions are then, &ldquo;<strong>How close can we get to the ideal confidence intervals?, With what kind of mean estimators?, and Whether such estimators are computationally tractable?</strong>&rdquo; There has been great strides in this line of research recently; we highly recommend the great survey papers [<a href="#LM19">LM19</a>] and [<a href="#DK19">DK19</a>] for more details on these recent advances as well as the historical background.</p> 
             
              <p> In [<a href="#Bah20">Bah20</a>], we propose a competing mean estimator (in a separable Banach space) based on <i>empirical characteristic function</i>. For the sake of simpler exposition, we explain the ideas in the usual Euclidean setting. Given i.i.d. copies \(X_1,\dotsc,X_n\) of a random vector \(X\) in \(\mathbb{R}^d\), we want to estimate the mean of \(X\) denoted, by \(\mu_\star\) with error measured in the \(\ell_2\)-norm. <strong>The only restriction on the law of \(X\) is that the corresponding covariance matrix, \(\varSigma_\star\), exists and is bounded. Furthermore, we may face <i>strong contamination</i> of the samples, where for some \(\eta\in(0,1/2)\), an adversary may arbitrarily manipulate up to \(\eta n\) samples.</strong></p>
              <p> With the empirical characteristic function denoted by
                \[
                \varphi_n(w) = \frac{1}{n}\sum_{i=1}^n \exp(\langle w, X_i\rangle)\nonumber\,,
                \]
                our proposed estimator is 
                \[\widehat{\mu} \in \operatorname*{argmin}_\mu \max_{w\,:\,{\Vert w\Vert}\le r_n} \langle w, \mu\rangle - \mathrm{Im}(\varphi_n(w))\,,\nonumber\]
                for appropriately chosen radius \(r_n>0\). In particular, in the non-adversarial setting, we show that by choosing \(r_n = \frac{22\log(1/\delta)}{n\epsilon}\), with probability at least \(1-\delta\) the estimator achieves the bound \(\|\hat{\mu}-\mu_\star\|\le \epsilon\), if the prescribed \(\epsilon\) obeys \[\begin{align*} \epsilon & \ge \max\Big\{96\sqrt{\frac{\mathrm{tr}(\varSigma_\star)}{n}}+12\sqrt{\frac{\|\varSigma_\star\|_{\mathrm{op}}\log(1/\delta)}{n}},\\ & \hphantom{\max\Big\{ }\quad9{\left(\frac{\log(1/\delta)}{n}\right)}^{2/3}\|\mu_\star\|\Big\}\,.\end{align*}\] We also prove a similar statement in the adversarial setting. Basically, the effect of contamination in terms of the contamination parameter \(\eta\in(0,1/2)\) is an additional term of order \(\sqrt{\eta\|\varSigma_\star\|_{\mathrm{op}}}\) that appears in the confidence interval. Both of the derived confidence intervals have an undesirable dependence on \(\|\mu_\star\|\) that, even though vanishing at a rate \(n^{-2/3}\), prevents achieving the ideal purely sub-Gaussian rate. In [<a href="#Bah20">Bah20</a>] we discuss ways to diminish this nuisance. Furthermore, we discuss how the estimator can be made oblivious to the accuracy level \(\epsilon\). The main drawback of the estimator is that, superficially, it does not appear to be computationally tractable.
              </p>
              <h4>Bibliography</h4>
              <p>
                <span class="anchor" id="Bah20"></span>
                [Bah20] S. Bahmani, <span class="semibold">&ldquo;<a class="marked"
                    href="https://arxiv.org/pdf/2004.02287.pdf">Nearly optimal robust mean estimation via empirical characteristic function</a>,&rdquo;</span> arXiv preprint, 2020.
              </p>
              <p>
                <span class="anchor" id="LM19"></span>
                [LM19] G. Lugosi and S. Mendelson, <span class="semibold">&ldquo;<a href="https://doi.org/10.1007/s10208-019-09427-x">Mean estimation and regression under heavy-tailed distributions: A survey</a>,&rdquo;</span> Foundations of Computational Mathematics, 19(5):1145&ndash;1190, 2019.
              </p>
              <p>
                <span class="anchor" id="DK8"></span>
                [DK8] I. Diakonikolas and D. Kane, <span class="semibold">&ldquo;<a href="https://arxiv.org/abs/1911.05911">Recent advances in algorithmic high-dimensional robust statistics</a>,&rdquo;</span> arXiv preprint, 2019.
              </p>
          </div>
  
        <!-- RNN via Convex Programming -->
        <button class="accordion active">
          <div class="accordion-title marked">A Convex Programming Approach to Estimation in Recurrent Neural Nets</div>
        </button>
        <div class="panel no-js">
          Consider the trajectory \((\boldsymbol{u}_0, \boldsymbol{x}_0=\boldsymbol{0}),(\boldsymbol{u}_1,
          \boldsymbol{x}_1),\dotsc,(\boldsymbol{u}_T, \boldsymbol{x}_T)\in\mathbb{R}^n\times\mathbb{R}^p\) of input-state pairs generated
          by the recursion
           \[
             \begin{align}
             \boldsymbol{x}_{t}=\nabla
            f(\boldsymbol{A}_\star\boldsymbol{x}_{t-1}+\boldsymbol{B}_\star\boldsymbol{u}_{t-1})\,,\label{eq:RNN}
            \end{align}
            \]
        
          where \(\nabla f\) is the gradient of a given <i>differentiable</i> and <i>convex</i> function \(f\), and
          \(\boldsymbol{A}_\star\in\mathbb{R}^{n\times n}\), \(\boldsymbol{B}_\star\in\mathbb{R}^{n\times p}\) are model parametrs
          we want to estimate. This observation model generalizes some common <i>recurrent neural net</i> (RNN) models. In particular,
          an RNN with <i>Rectified Linear Unit</i> (ReLU) activations can be expressed in the above form by choosing
          \[
          \begin{align*}
           f(\boldsymbol{x})\overset{\mathrm{\scriptscriptstyle def}}{=} \frac{1}{2}\sum_{i=1}^n
          {\left(\max\left\{x_i,0\right\}\right)}^2\,.
          \end{align*}
          \]
          
          Let \(\beta>0\) be a sufficiently large normalizing parameter. We can express the recursion \eqref{eq:RNN} equivalently in
          terms of
          \[
            \begin{align*}
            \boldsymbol{C}_\star=\begin{bmatrix} \boldsymbol{A}_\star & \beta^{-1}\boldsymbol{B}_\star\end{bmatrix}
            \end{align*}
          \]
           and
          \[
            \begin{align*}
            \boldsymbol{z}_t=\begin{bmatrix}\boldsymbol{x}_t\\ \beta\boldsymbol{u}_t\end{bmatrix}\,.
            \end{align*}
          \]
          Namely, for \(t\ge 1\) we have the recursion
          \[
            \begin{align*}
              \boldsymbol{z}_t=\begin{bmatrix}\nabla f(\boldsymbol{C}_\star\boldsymbol{z}_{t-1})\\
              \beta \boldsymbol{u}_t \end{bmatrix}\,,
            \end{align*}
          \]
          and at \(t=0\) we have
          \[
            \begin{align*}
              \boldsymbol{z}_0=\begin{bmatrix} \boldsymbol{0}\\
              \beta \boldsymbol{u}_0 \end{bmatrix}\,.
            \end{align*}
          \]
          It follows from the convexity of \(f\) and \eqref{eq:RNN} that
          \[
          \begin{align*}
            f(\boldsymbol{C}\boldsymbol{z}_{t-1})-\langle\boldsymbol{x}_t, \boldsymbol{C}\boldsymbol{z}_{t-1}\rangle \ge
            f(\boldsymbol{C}_\star\boldsymbol{z}_{t-1})-\langle\boldsymbol{x}_t, \boldsymbol{C}_\star\boldsymbol{z}_{t-1}
            \rangle\,.
          \end{align*}
          \]
          Therefore, we can formulate a convex program as an estimator for \(\boldsymbol{C}_\star\):
          \[
            \begin{align*}
              \widehat{\boldsymbol{C}} = \operatorname*{argmin}_{\boldsymbol{C}}\ \sum_{t=1}^T
          f(\boldsymbol{C}\boldsymbol{z}_{t-1})-\langle\boldsymbol{x}_t, \boldsymbol{C}\boldsymbol{z}_{t-1}\rangle\,.
            \end{align*}
          \]
          
          In [<a href="#BR19">BR19</a>], under some additional assumptions on the nonlinearity and the distribution of the input, we showed that,
          with high probability, a time horizon \(T\) that scales with \(n+p\), up to logarithmic factors, suffices for exact recovery
          of \(\boldsymbol{C}_\star\) from a single trajectory.
          
          <h4>Bibliography</h4>
          <p>
            <span class="anchor" id="BR19"></span>
            [BR19] S. Bahmani and J. Romberg, <span class="semibold">&ldquo;<a class="marked" href="https://arxiv.org/abs/1908.09915">Convex programming for estimation in nonlinear recurrent
            models</a>,&rdquo;</span> 2019.
          </p>
        </div>
        <!-- Anchored Regression -->
        <button class="accordion active">
          <div class="accordion-title marked">Regression via Convex Programming (II): Non-convex Observations</div>
        </button>
        <div class="panel no-js">
          We provide a computationally tractable method to address a large class of parametric regression problems that
          involve <strong>difference of convex</strong> (DC) observation functions. This class of regression problems
          are much broader than those with the ``convex observation'' model considered in Part (I) because many
          non-convex functions can be written in DC form as well.
          In the considered observation model, we measure \(\boldsymbol{x}_\star\in\mathbb{R}^N\) indirectly as
          \begin{align*}
          y_m & = f_m(\boldsymbol{x}_\star) - g_m(\boldsymbol{x}_\star) + \xi_m\,, & m=1,\dotsc,M\,,
          \end{align*}
          where the pairs of functions \((f_m,\,g_m)\) are i.i.d. copies of a pair of random <i>convex</i> functions \((f,\,g)\), and \(\xi_m\)
          denotes the noise as in Part (I). Assuming that we have access to
          \begin{align}
          \boldsymbol{a}_0 & \approx \frac{1}{2M}\sum_{m=1}^M\nabla f_m(\boldsymbol{x}_\star) + \nabla
          g_m(\boldsymbol{x}_\star)\,,\label{a0-approx}
          \end{align}
          where the approximation has a certain precise form, then we formulate the estimator
          \(\widehat{\boldsymbol{x}}\) of \(\boldsymbol{x}_\star\) as
          \begin{align*}
          \widehat{\boldsymbol{x}} & = \operatorname*{argmax}_{\boldsymbol{x}}\ \langle\boldsymbol{a}_0,
          \boldsymbol{x}\rangle - \frac{1}{M}\sum_{m=1}^M \max \left\lbrace f_m(\boldsymbol{x}) - y_m,
          g_m(\boldsymbol{x}) \right\rbrace\,.
          \end{align*}
          Under mild <i>non-degeneracy</i> and <i>regularity</i> assumptions, we established in [<a href="#Bah18">Bah18</a>]
          a sample complexity for guaranteed accuracy of \(\widehat{\boldsymbol{x}}\) using a PAC-Bayesian analysis. To
          provide a concrete example we also studied <i>bilinear regression</i> with standard Gaussian factors as a
          special case; for this problem we also described a method to create a vector \(\boldsymbol{a}_0\) that
          provably satisfies the required approximation \eqref{a0-approx}.
          <h4>Bibliography</h4>
          <p><span class="anchor" id="Bah18"></span> [Bah18] S. Bahmani, <span class="semibold">&ldquo;<a class="marked"
                  href="https://projecteuclid.org/euclid.ejs/1560909647">Estimation from nonlinear observations via
                  convex programming, with application to bilinear regression</a>,&rdquo;</span> <i>Elect. J.
              Statistics</i>, 13(1): 1978&ndash;2011, 2019.</p>

        </div>
        <button class="accordion active">
          <div class="accordion-title marked">Regression via Convex Programming (I): Convex Observations</div>
        </button>
        <div class="panel no-js">
          <h4>Problem statement</h4>
          <p>Consider the problem of estimating an \(N\)-dimensional signal \(\boldsymbol{x}_\star\) from
            observations of the form
            <span class="eqanchor">
              \begin{align}
              y_m & = f_m(\boldsymbol{x}_\star) + \xi_m\,, & m=1,2,\dotsc,M\,,
              \label{cvxreg}
              \end{align}
            </span>
            where the functions \(f_m\) are i.i.d. copies of a randomly drawn convex function \(f\), and the noise terms
            are represented by \(\xi_m\). Despite the convexity assumption on the functions \(f_m\), the observation
            model \eqref{cvxreg} is quite general and includes many standard statistical models as the special case
            including <i>generalized linear models</i> and <i>single hidden layer neural nets</i>. I addressed the
            general regression problem \eqref{cvxreg} in [<a href="#BR17b">BR17b</a>] building upon ideas I developed in
            [<a href="#BR17a">BR17a</a>] to address a special case often known as <i>phase retrieval</i>. It is more
            illustrative to begin my explanation with this special case as well.</p>
          <h4>A new approach to phase retrieval</h4>
          <p> The problem of phase retrieval appears in areas such as imaging and optics where the sensors often measure
            only intensities; the sign- (or phase-) information is generally lost. The observation model in the phase
            retrieval problem, assuming no noise in the measurements, can be abstracted as the system of quadratic
            equations
            \begin{equation*}
            \begin{aligned}
            y_1 & = |\boldsymbol{a}_1^*\boldsymbol{x}_\star|^2\\
            y_2 & = |\boldsymbol{a}_2^*\boldsymbol{x}_\star|^2\\
            \vdots & \qquad\vdots\\
            y_M & = |\boldsymbol{a}_M^*\boldsymbol{x}_\star|^2\,,
            \end{aligned}
            \end{equation*}
            for \(\boldsymbol{x}_\star\in\mathbb{C}^N\). Clearly, this model corresponds to \eqref{cvxreg} with
            \(f_m(\boldsymbol{x}) = |\boldsymbol{a}_m^*\boldsymbol{x}|^2\) for i.i.d. draws of \(\boldsymbol{a}_m\), and
            \(\xi_m=0\). In [<a href="#BR17a">BR17a</a>], I formulated a new estimator for this problem as the convex
            program
            <span class="eqanchor">
              \begin{equation}
              \begin{aligned}
              \operatorname*{argmax}_{\boldsymbol{x}}\ & \mathrm{Re}(\boldsymbol{a}_0^*\boldsymbol{x}) & \\
              \text{subject to}\ & |\boldsymbol{a}_m^*\boldsymbol{x}|^2 \le y_m, & m=1,2,\dotsc,M\,,
              \end{aligned} \label{linmax}
              \end{equation}
            </span>
            where \(\boldsymbol{a}_0\) denotes an &ldquo;anchor vector&rdquo; that obeys
            \begin{equation*}
            |\boldsymbol{a}_0^*\boldsymbol{x}_\star| \ge \delta \left\lVert\boldsymbol{a}_0\right\rVert_2
            \left\lVert\boldsymbol{x}_\star\right\rVert_2\,,
            \end{equation*}
            for some absolute constant \(\delta \in (0,1]\). The anchor vector can be constructed from random
            observations similar to initializations in some non-convex methods (e.g., <a
              href="http://doi.org/10.1109/TIT.2015.2399924">Wirtinger Flow</a>); the details can be found in the paper.
            Here, I explain the geometric intuition behind \eqref{linmax}
            in the case of real-valued variables for clarity. As illustrated in Figure <a href="#slabs">1</a>, each of
            the constraints in \eqref{linmax} form a slab of feasible points, whose intersection is a convex polytope
            \(\mathcal{K}\). Clearly, \(\boldsymbol{x}_\star\) is an extreme point of \(\mathcal{K}\). The solution to
            \eqref{linmax} is also always an extreme point of \(\mathcal{K}\), since it is a solution to linear
            maximization over the convex body \(\mathcal{K}\). The key observation is that if the anchor vector
            \(\boldsymbol{a}_0\) has a non-trivial component in the direction of \(\boldsymbol{x}_\star\), we can expect
            that the extreme point found by \eqref{linmax} coincides with \(\boldsymbol{x}_\star\).</p>
          <span id="slabs" class="anchor"></span>
          <figure>
            <img style="margin-left: auto; margin-right: auto; display: block;" src="slabs.gif"
              alt="geometry of intersecting slabs" width="90%">
            <figcaption><strong>Figure 1.</strong> Geometry of slabs intersecting at \(\boldsymbol{x}_\star\) and their
              positioning with respect to the anchor \(\boldsymbol{a}_0\)</figcaption>
          </figure>
          <p>Using classic results from <em>statistical learning theory</em>, I showed that with high probability
            \begin{equation*}
            M = C_\delta N
            \end{equation*}
            independent random measurements suffice to recover \(\boldsymbol{x}_\star\)
            using \eqref{linmax}, with \(C_\delta\) being an absolute constant depending only on \(\delta\). Robustness
            under specific noise models is also addressed in the paper.</p>
          <h4>Why does \eqref{linmax} matter?</h4>
          <p>
            Previous convex relaxations for phase retrieval (e.g., <a
              href="http://dx.doi.org/10.1002/cpa.21432">PhaseLift</a>, and <a
              href="http://doi.org/10.1007/s10107-013-0738-9">PhaseCut</a>) were based on the idea of <i>lifting</i> and
            <i>semidefinite programming</i> (SDP). While lifting-based methods are technically computationally
            tractable, their dependence on SDP prohibits their scalability. In contrast, \eqref{linmax} operates in the
            natural domain of the problem and competes with non-convex methods for phase retrieval (e.g., <a
              href="http://doi.org/10.1109/TIT.2015.2399924">Wirtinger Flow</a>). It also benefits from versatility,
            flexibility, and robustness that is associated with convex programming. More importantly, as discussed
            below, the principles used in formulation and analysis of \eqref{linmax} apply in a more general setting.
          </p>
          <h4>What about the general case \eqref{cvxreg}?</h4>
          <p>In [<a href="#BR17b">BR17b</a>], I proposed the convex program
            <span class="eqanchor">
              \begin{equation}
              \begin{aligned}
              \operatorname*{argmax}_{\boldsymbol{x}}\ & \langle \boldsymbol{a}_0,\boldsymbol{x}\rangle \\
              \text{subject to}\ & \sum_{m=1}^M \max\{f_m(\boldsymbol{x})-y_m, 0\} \le \varepsilon\, ,
              \end{aligned} \label{anchored_reg}
              \end{equation}
            </span>
            where \(\boldsymbol{a}_0\) is an &ldquo;anchor vector&rdquo; that obeys
            \begin{equation*}
            \langle\boldsymbol{a}_0,\boldsymbol{x}_\star\rangle \ge \delta
            \left\lVert\boldsymbol{a}_0\right\rVert_2\left\lVert\boldsymbol{x}_\star\right\rVert_2\,,
            \end{equation*}
            as an estimator for the general regression problem \eqref{cvxreg}. Some schemes for constructing the anchor
            from the measurements are described in the paper, but we omit the discussion for brevity. Furthermore, to
            avoid technical details, here I state the main result of the paper (i.e., <a target="_blank"
              href="https://arxiv.org/pdf/1702.05327.pdf#page=9">Theorem 2.1</a>) on the sample complexity of
            \eqref{anchored_reg} in an informal way. In the proved bound, there are two important quantities. The first
            quantity, \(\mathfrak{C}_M(\mathcal{A}_\delta)\), measures the &ldquo;size&rdquo; of a set
            \(\mathcal{A}_\delta\) that depends only on \(\boldsymbol{x}_\star\) and \(\delta\), with respect to the
            randomness in the gradients \(\nabla f_m(\boldsymbol{x}_\star)\). The second quantity,
            \(p_\tau(\mathcal{A}_\delta)\), is some measure of the &ldquo;eccentricity&rdquo; of the random vector
            \(\nabla f_m(\boldsymbol{x}_\star)\) with respect to the set \(\mathcal{A}_\delta\) in terms of a parameter
            \(\tau\). Ignoring some details, the result established in [<a href="#BR17b">BR17b</a>] basically states
            that
            \begin{equation*}
            M \gtrsim \left(\frac{\mathfrak{C}_M(\mathcal{A}_\delta)}{\tau p_\tau(\mathcal{A}_\delta)}\right)^2\,,
            \end{equation*}
            measurements are sufficient to guarantee that \eqref{anchored_reg} yields an accurate estimate.
          </p>

          <h4>Bibliography</h4>
          <p> <span class="anchor" id="BR17a"></span>[BR17a] S. Bahmani and J. Romberg, <span class="semibold">&ldquo;<a
                  class="marked" href="http://proceedings.mlr.press/v54/bahmani17a.html">Phase
                  retrieval meets statistical learning theory: A flexible
                  convex relaxation</a>,&rdquo;</span> <em>In Proceedings of the
              20th International Conference on Artificial Intelligence
              and Statistics (AISTATS'17)</em>, vol. 54 of Proceedings
            of Machine Learning Research , pp. 252&ndash;260.</p>
          <p><span class="anchor" id="BR17b"></span> [BR17b] S. Bahmani and J. Romberg, <span class="semibold">&ldquo;<a
                  class="marked" href="https://arxiv.org/abs/1702.05327">Solving equations of random convex functions
                  via anchored regression</a>,&rdquo;</span> <i>Foundations of Computational Mathematics</i>,
            19(4):813&ndash;841, 2019.</p>
        </div>
        <!-- Alg. Connectivity under Site Perc. -->
        <button class="accordion active">
          <div class="accordion-title marked">Network Connectivity Under Random Node Removal</div>
        </button>
        <div class="panel no-js">
          <p> An important characteristic of networks in many applications is their connectivity which is often a
            crucial factor in the performance of the network. An interesting and important problem is then to measure
            robustness of the connectivity under some form of perturbation of the network. <i>Site percolation</i>, or
            simply random removal of nodes as illustrated in Figure <a href="#SitePerc">2</a>, is one of these
            perturbation models that is studied in mathematics and statistical physics.
            <span id="SitePerc" class="anchor"></span>
            <figure>
              <img style="margin-left: auto; margin-right: auto; display: block;" alt="" src="percolation.gif"
                width="90%">
              <figcaption><strong>Figure 2.</strong> Under a site percolation, the surviving subgraph is no longer
                connected</figcaption>
            </figure>
          </p>
          <p><i>Algebraic connectivity</i> of a graph is a an analytical measure of connectivity that is also related to
            the conductance of the graph through the Cheeger's inequality. Formally, the algebraic connectivity of a
            graph with the adjacency matrix \(\boldsymbol{A}\) can be defined as <i>the second smallest eigenvalue</i>
            of the graph Laplacian \(\boldsymbol{L} = \boldsymbol{D} - \boldsymbol{A}\) where
            \(\boldsymbol{D}=\mathrm{diag}(\boldsymbol{A}\boldsymbol{1})\) is the diagonal matrix of the vertex degrees.
            In [<a href="#BRT18">BRT18</a>], using tools from random matrix theory I derived a lower bound for algebraic
            connectivity of a graph that survives from a generally non-homogeneous site percolation. In the special case
            of homogeneous site percolation over a certain class of regular graphs, our analytical result virtually
            coincides with the state-of-the-art that is established using refined combinatorial arguments.</p>
          <h4>Bibliography</h4>
          <p> <span class="anchor" id="BRT18"></span>[BRT18] S. Bahmani, J. Romberg, and P. Tetali, <span
              class="semibold">&ldquo;<a class="marked" href="https://doi.org/10.1109/TNSE.2017.2757762">Algebraic
                  connectivity under site percolation in finite weighted graphs</a>,&rdquo;</span> <em>IEEE Trans. Network
              Science and Engineering</em>, 5(2):86&ndash;91, 2018.</p>
        </div>

      </div>
      
      <span id="publications" class="anchor"></span>
      <div class="item">
        <h2>Publications</h2>
        <ul class="paper">
          <li class="sub"> In Review/Revision</li>
          <li class="jrn"> Journal Paper</li>
          <li class="cnf"> Conference Paper</li>        
          <li class="rpt"> Technical Report</li>
        </ul>
        <h4>In Review/Revision</h4>
        <ol class="paper begin">
			<li class="sub"> S. Bahmani, <span class="semibold">&ldquo;Instance-dependent uniform tail bounds for empirical processes,&rdquo;</span> 2022.<br><a class="hlink marked" href="http://arxiv.org/abs/2209.10053">arXiv</a></li>
			<li class="sub"> B. Ancelin, S. Bahmani, J. Romberg, <span class="semibold">&ldquo;Decentralized feature-distributed optimization for generalized linear models,&rdquo;</span> in review, 2021.<br><a class="hlink marked" href="https://arxiv.org/abs/2110.15283">arXiv</a></li>
			<li class="sub"> S. Kim, S. Bahmani, K. Lee, <span class="semibold">&ldquo;Max-linear regression by scalable and guaranteed convex programming,&rdquo;</span> in review, 2021.<br><a class="hlink marked" href="https://arxiv.org/abs/2103.07020">arXiv</a></li>
        </ol>
	<h4>2021</h4>
	<ol class="paper">
		<li class="jrn"> S. Bahmani, K. Lee, <span class="semibold">&ldquo;Low-rank matrix estimation from rank-one projections by unlifted convex optimization&rdquo;</span>, to appear in the <i>SIAM J. on Matrix Analysis and Applications</i>, 2021.<br><a class="hlink marked" href="https://arxiv.org/abs/2004.02718">arXiv</a></li>
		<li class="jrn"> S. Bahmani, <span class="semibold">&ldquo;Nearly optimal robust mean estimation via empirical characteristic function,&rdquo;</span> <i>Bernoulli</i>, 27(3): 2139&ndash;2158, 2021.</br><a class="hlink marked" href="https://arxiv.org/abs/2004.02287">arXiv</a><a class="hlink marked" href="http://dx.doi.org/10.3150/20-BEJ1304">Proj. Euclid</a></li>
	</ol>
	<h4>2020</h4>
	<ol class="paper">
	  <li class="jrn"> S. Bahmani, J. Romberg, <span class="semibold">&ldquo;Convex programming for estimation in
                nonlinear recurrent models,&rdquo;</span> <i>Journal of Machine Learning Research</i>, (235):1&ndash;20, 2020.</br><a class="hlink marked"
              href="https://arxiv.org/abs/1908.09915">arXiv</a><a class="hlink marked"
              href="https://jmlr.org/papers/v21/19-723.html">JMLR</a><a class="hclink marked"
              href="./Codes/Convex-RNN.ipynb" download>Code</a></li>
          <li class="jrn"> K. Lee, S. Bahmani, J. Romberg, Y. Eldar, <span class="semibold">&ldquo;Phase retrieval of
                low-rank matrices by anchored regression,&rdquo;</span> <i>Information and Inference: A
              Journal of the IMA</i>, 2020.</br><a class="hlink marked" href="https://arxiv.org/abs/1910.11477">arXiv</a><a class="hlink marked" href="https://doi.org/10.1093/imaiai/iaaa018">Oxford Journals</a></li>
	</ol>
        <h4>2019</h4>
        <ol class="paper">
          <li class="jrn"> S. Bahmani, <span class="semibold">&ldquo;Estimation from nonlinear observations via convex
                programming, with application to bilinear regression,&rdquo;</span> <i>Electronic J. of Statistics</i>,
            13(1): 1978&ndash;2011. 2019.</br><a class="hlink marked" href="https://arxiv.org/abs/1806.07307">arXiv</a><a
              class="hlink marked" href="http://dx.doi.org/10.1214/19-EJS1567">Proj. Euclid</a></li>
        </ol>
        <h4>2018</h4>
        <ol class="paper">
          <li class="jrn"> S. Bahmani and J. Romberg, <span class="semibold">&ldquo;Solving
                equations of random convex functions via anchored regression,&rdquo;</span>
            <i>Foundations of Computational Mathematics</i>, 19(4):813&ndash;841, 2019.</br><a class="hlink marked"
              href="https://arxiv.org/abs/1702.05327">arXiv</a><a class="hlink marked"
              href="https://doi.org/10.1007/s10208-018-9401-4">Springer</a></li>
          <li class="jrn"> S. Bahmani, J. Romberg, P. Tetali, <span class="semibold">&ldquo;Algebraic
                connectivity under site percolation in finite weighted graphs,&rdquo;</span>
            <i>IEEE Trans. on Network Science and Engineering</i>, 5(2):86&ndash;91, 2018.
            <a class="hlink marked" href="https://arxiv.org/abs/1612.05986">arXiv</a><a class="hlink"
              href="https://doi.org/10.1109/TNSE.2017.2757762">IEEEXplore</a></li>
        </ol>
        <h4>2017</h4>
        <ol class="paper">
          <li class="jrn"> S. Bahmani and J. Romberg, <span class="semibold">&ldquo;A
                flexible convex relaxation for phase retrieval,&rdquo;</span> <i>Electronic Journal of Statistics,</i>
            11(2):5254&ndash;5281, 2017. (This is
            an extended version of the AISTATS’17 paper.)<br/><a class="hlink marked"
              href="https://doi.org/10.1214/17-EJS1378SI">Proj. Euclid</a></li>
          <li class="cnf"> S. Bahmani and J. Romberg, <span class="semibold">&ldquo;Phase
                retrieval meets statistical learning theory: A flexible convex
                relaxation,&rdquo;</span> <em>In Proceedings of the 20th
              International Conference on Artificial Intelligence and
              Statistics (AISTATS'17)</em>, vol. 54 of Proceedings of Machine
            Learning Research , pp. 252&ndash;260. (<strong>Best paper award</strong>)
            <a class="hlink marked" href="http://arxiv.org/abs/1610.04210">arXiv</a><a class="hlink marked"
              href="http://proceedings.mlr.press/v54/bahmani17a.html">PMLR</a></li>
        </ol>
        <h4>2016</h4>
        <ol class="paper">
          <li class="jrn"> S. Bahmani and J. Romberg, <span class="semibold">&ldquo;Near-optimal
                estimation of simultaneously sparse and low-rank matrices from
                nested linear measurements,&rdquo;</span> <i>Information and
              Inference: A Journal of the IMA</i> 5(3):331&ndash;351, 2016.</br><a class="hlink"
              href="http://arxiv.org/abs/1506.08159">arXiv</a><a class="hlink"
              href="http://dx.doi.org/10.1093/imaiai/iaw012">Oxford
              Journals</a></li>
          <li class="jrn"> S. Bahmani, P. Boufounos, and B. Raj, <span class="semibold">&ldquo;Learning
                model-based sparsity via projected gradient descent,&rdquo;</span>
            <i>IEEE Trans. Info. Theory</i>, 62(4):2092&ndash;2099, 2016.</br><a class="hlink"
              href="http://arxiv.org/abs/1209.1557">arXiv</a><a class="hlink"
              href="http://dx.doi.org/10.1109/TIT.2016.2515078">IEEEXplore</a></li>
        </ol>
        <h4>2015</h4>
        <ol class="paper">
          <li class="cnf"> S. Bahmani and J. Romberg, <span class="semibold">&ldquo;Sketching
                for simultaneously sparse and low-rank covariance matrices,&rdquo;</span>
            in <em>Computational Advances in Multi-Sensor Adaptive Processing
              (CAMSAP'15), IEEE 6th International Workshop on</em>, pp.
            357&ndash;360, Cancun, Mexico, Dec. 2015.<a class="hlink" href="http://arxiv.org/abs/1510.01670">arXiv</a><a
              class="hlink" href="http://dx.doi.org/10.1109/CAMSAP.2015.7383810">IEEEXplore</a></li>
          <li class="cnf"> S. Bahmani and J. Romberg, <span class="semibold">
              &ldquo;Efficient compressive phase retrieval with constrained
                sensing vectors,&rdquo;</span> in <em>Advances in Neural
              Information Processing Systems (NIPS'15),</em> vol. 28, pp.
            523&ndash;531, Montr&eacute;al, Canada, Dec. 2015.</br><a class="hlink"
              href="http://arxiv.org/abs/1507.08254">arXiv</a><a class="hlink"
              href="https://papers.nips.cc/paper/6021-efficient-compressive-phase-retrieval-with-constrained-sensing-vectors">NIPS</a>
          </li>
          <li class="jrn"> S. Bahmani and J. Romberg, <span class="semibold">&ldquo;Lifting
                for blind deconvolution in random mask imaging:
                Identifiability and convex relaxation,&rdquo;</span> <em>SIAM
              Journal on Imaging Sciences</em>, 8(4):2203&ndash;2238, 2015.</br><a class="hlink"
              href="http://arxiv.org/abs/1501.00046">arXiv</a><a class="hlink"
              href="http://dx.doi.org/10.1137/141002165">SIAM</a></li>
          <li class="jrn"> S. Bahmani and J. Romberg, <span class="semibold">&ldquo;Compressive
                deconvolution in random mask imaging,&rdquo;</span> <em>IEEE
              Trans. on Computational Imaging</em>, 1(4):236&ndash;246, 2015.</br><a class="hlink"
              href="http://arxiv.org/abs/1412.7890">arXiv</a><a class="hlink"
              href="http://dx.doi.org/10.1109/TCI.2015.2485941">IEEEXplore</a></li>
        </ol>
        <h4>2013</h4>
        <ol class="paper">
          <li class="jrn"> S. Bahmani, B. Raj, and P. T. Boufounos, <span class="semibold">&ldquo;Greedy
                sparsity-constrained optimization,&rdquo;</span> <i>Journal of
              Machine Learning Research</i>, 14(3):807&ndash;841, 2013.</br><a class="hlink"
              href="http://arxiv.org/abs/1203.5483">arXiv</a><a class="hlink"
              href="http://jmlr.csail.mit.edu/papers/v14/bahmani13a.html">JMLR</a><a class="hclink"
              href="./GraSP.html">Code</a></li>
          <li class="rpt"> S. Bahmani, P. Boufounos, and B. Raj, <span class="semibold">&ldquo;Robust
                1-bit compressive sensing via gradient support pursuit,&rdquo;</span>
            Apr. 2013.</br><a class="hlink" href="http://arxiv.org/abs/1304.6627">arXiv</a></li>
        </ol>
        <h4>2012</h4>
        <ol class="paper">
          <li class="jrn"> S. Bahmani, B. Raj, <span class="semibold">&ldquo;A
                unifying analysis of projected gradient descent for
                \(\ell_p\)-constrained least squares,&rdquo;</span> <em>Applied
              and Computational Harmonic Analysis</em>, 34(3):366&ndash;378, 2012.</br><a class="hlink"
              href="http://arxiv.org/abs/1107.4623">arXiv</a><a class="hlink"
              href="http://dx.doi.org/10.1016/j.acha.2012.07.004">Elsevier</a></li>
        </ol>
        <h4>2011</h4>
        <ol class="paper">
          <li class="cnf"> S. Bahmani, P. Boufonos, and B. Raj, <span class="semibold">&ldquo;Greedy
                sparsity-constrained optimization,&rdquo;</span> in <i>Conf.
              Record of the 45th Asilomar Conference on Signals, Systems, and
              Computers (ASILOMAR'11)</i>, pp. 1148&ndash;1152, Pacific Grove, CA,
            Nov. 2011.
            <!--<a href="./docs/Asilomar%2711.pdf">Preprint</a>, --><a class="hlink"
              href="http://dx.doi.org/10.1109/ACSSC.2011.6190194">IEEEXplore</a><a class="hlink"
              href="./slides/Asilomar-SBahmani.pdf">Slides</a><a class="hclink" href="./GraSP.html">Code</a> </li>
        </ol>
        <h4>2010</h4>
        <ol class="paper">
          <li class="jrn"> S. Bahmani, I. Bajić, and A. HajShirmohammadi, <span class="semibold">&ldquo;Joint decoding of
                unequally protected
                JPEG2000 images and Reed-Solomon codes,&rdquo;</span> <i>IEEE
              Trans. Image Processing</i>, 19(10):2693&ndash;2704, Oct. 2010.</br><a class="hlink"
              href="http://dx.doi.org/10.1109/TIP.2010.2049529">IEEEXplore</a></li>
        </ol>
        <h4>2009</h4>
        <ol class="paper">
          <li class="cnf"> S. Bahmani, I. Bajić, and A. HajShirmohammadi, <span class="semibold">&ldquo;Improved joint
                source channel decoding of
                JPEG2000 images and Reed-Solomon codes,&rdquo;</span> <i> Proc.
              IEEE ICC'09</i>, Dresden, Germany, Jun. 2009.</br><a class="hlink"
              href="http://dx.doi.org/10.1109/ICC.2009.5305946">IEEEXplore</a></li>
        </ol>
        <h4>2008</h4>
        <ol class="paper">
          <li class="cnf"> S. Bahmani, I. Bajić, A. HajShirmohammadi, <span class="semibold">&ldquo;Joint
                source channel decoding of JPEG2000 images with unequal loss
                protection,&rdquo;</span> <i>Proc. IEEE ICASSP'08</i>, pp.
            1365&ndash;1368, Las Vegas, NV, Mar. 2008.</br><a class="hlink"
              href="http://dx.doi.org/10.1109/ICASSP.2008.4517872">IEEEXplore</a></li>
        </ol>
        <h4>Thesis</h4>
        <ul style="margin-top: 0.5ex; list-style-type: square;">
          <li style="font-size: 95%;">S. Bahmani, <span class="semibold">Algorithms
              for sparsity-constrained optimization</span>, PhD dissertation,
            Department of Electrical &amp; Computer Engineernig, Carnegie
            Mellon University, Pittsburgh, PA, Feb. 2013.</br><a class="hlink" href="./Thesis.pdf">PDF</a></li>
          <!--<a class="hlink" href="./Defense.pdf">Slides</a>-->
          <!--
                    <li>S. Bahmani, <span class="semibold">Joint source-channel decoding of JPEG2000 images with unequal loss protection,</span> Master's thesis, School of Engineering Science, Simon Fraser University, Burnaby, British Columbia, Canada, Nov. 2008.</br><a class="hlink" href="http://ir.lib.sfu.ca/bitstream/1892/10578/1/etd4224.pdf">PDF</a></li>-->
        </ul>
      </div>
      <!-- #EndEditable -->
      <div>Icons made by <a href="http://www.freepik.com" title="Freepik">Freepik</a> from <a
          href="https://www.flaticon.com/" title="Flaticon">www.flaticon.com</a> is licensed by <a
          href="http://creativecommons.org/licenses/by/3.0/" title="Creative Commons BY 3.0" target="_blank">CC 3.0
          BY</a></div>
    </div>
  </div>
	<!-- Default Statcounter code for My Website
https://sohail-bahmani.github.io -->
<script type="text/javascript">
var sc_project=12201095; 
var sc_invisible=1; 
var sc_security="3c36772c"; 
var sc_https=1; 
var sc_remove_link=1; 
</script>
<script type="text/javascript"
src="https://www.statcounter.com/counter/counter.js"
async></script>
<noscript><div class="statcounter"><img class="statcounter"
src="https://c.statcounter.com/12201095/0/3c36772c/1/"
alt="Web Analytics Made Easy -
StatCounter"></div></noscript>
<!-- End of Statcounter Code -->
</body>
<!-- #EndTemplate -->
<footer>
  <script src="accordion.js"></script>
</footer>

</html>
